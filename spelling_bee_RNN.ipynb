{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Spelling bee challenge\n",
    "\n",
    "In this notebook we are going to try and learn a model that can take in the pronunciation of a word as a list of phonemes, and try to spell it.  On the one hand this is easier than something like speech to text.  However one difficulty is this model will be evaluated on how well it can spell words that it has never seen before.  This is not a completely well-posed question, as there are often several reasonable spellings, and indeed, some words have [homonyms](https://en.wikipedia.org/wiki/Homonym) with different spellings.  I hope in future to try and give the model more information (e.g. country of origin), as in a real spelling bee to see if it can improve its guesses.  \n",
    "\n",
    "## Data Stuff (not interesting)\n",
    "\n",
    "We take our data set from [The CMU pronouncing dictionary](https://en.wikipedia.org/wiki/CMU_Pronouncing_Dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example line of file : \n",
      "SPERBER  S P ER1 B ER0\n"
     ]
    }
   ],
   "source": [
    "cmu_dict_raw = open(\"cmudict-0.7b\").read()\n",
    "\n",
    "first_line = \"A  AH0\"\n",
    "last_line = \"ZYWICKI  Z IH0 W IH1 K IY0\"\n",
    "\n",
    "lines = cmu_dict_raw.split(\"\\n\")\n",
    "\n",
    "for i, l in enumerate(lines):\n",
    "    if l == first_line:\n",
    "        first_index = i\n",
    "    if l == last_line:\n",
    "        last_index = i\n",
    "        \n",
    "print \"Example line of file : \"\n",
    "print lines[113150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get dictionaries to convert between indexes and letters/phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phonemes = set()\n",
    "\n",
    "for l in lines[first_index : last_index + 1]:\n",
    "    word, pronounce = l.split(\"  \")\n",
    "    for phoneme in pronounce.split():\n",
    "        phonemes.add(phoneme)\n",
    "        \n",
    "sorted_phonemes = [\"_\"] + sorted(list(phonemes))\n",
    "\n",
    "index_to_phoneme = dict(enumerate(sorted_phonemes))\n",
    "phoneme_to_index = dict((v, k) for k,v in index_to_phoneme.items())\n",
    "\n",
    "index_to_letter = dict(enumerate(\"_abcdefghijklmnopqrstuvwxyz\"))\n",
    "letter_to_index = dict((v, k) for k,v in index_to_letter.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "pronounce_dict = {}\n",
    "\n",
    "for l in lines[first_index : last_index + 1]:\n",
    "    word, phone_list = l.split(\"  \")\n",
    "    pronounce_dict[word.lower()] = [phoneme_to_index[p] for p in phone_list.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biggest word in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supercalifragilisticexpialidocious\n",
      "[55, 64, 53, 26, 42, 6, 43, 7, 32, 54, 5, 41, 7, 43, 37, 55, 57, 35, 42, 25, 42, 55, 53, 38, 6, 43, 7, 21, 48, 56, 7, 55]\n"
     ]
    }
   ],
   "source": [
    "max_k = max([len(k) for k,v in pronounce_dict.items()])\n",
    "max_v = max([len(v) for k,v in pronounce_dict.items()])\n",
    "for k,v in pronounce_dict.items():\n",
    "    if len(k) == max_k or  len(v) == max_v:\n",
    "        print k\n",
    "        print v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get rid of words that are too long, or that have punctuation or spaces in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133779\n"
     ]
    }
   ],
   "source": [
    "bad_ct = set()\n",
    "\n",
    "letters = set(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "print len(pronounce_dict)\n",
    "for k, v in pronounce_dict.items():\n",
    "    if len(k) < 5 or len(k) > 15:\n",
    "        del pronounce_dict[k]\n",
    "        continue\n",
    "    for c in k:\n",
    "        if c not in letters:\n",
    "            del pronounce_dict[k]\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split lines into words, phonemes, convert to indexes (with padding), split into training, validation, test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pairs = np.random.permutation(list(pronounce_dict.keys()))\n",
    "\n",
    "input_ = np.zeros((len(pairs), 16))\n",
    "labels_ = np.zeros((len(pairs), 15))\n",
    "\n",
    "for i, k in enumerate(pairs):\n",
    "    v = pronounce_dict[k]\n",
    "    k = k + \"_\" * (15 - len(k))\n",
    "    v = v + [0] * (16 - len(v))\n",
    "    for j, n in enumerate(v):\n",
    "        input_[i][j] = n\n",
    "    for j, letter in enumerate(k):\n",
    "        labels_[i][j] = letter_to_index[letter]\n",
    "        \n",
    "input_ = input_.astype(np.int32)\n",
    "labels_ = labels_.astype(np.int32)\n",
    "\n",
    "input_test   = input_[:10000]\n",
    "input_val    = input_[10000:20000]\n",
    "input_train  = input_[20000:]\n",
    "labels_test  = labels_[:10000]\n",
    "labels_val   = labels_[10000:20000]\n",
    "labels_train = labels_[20000:]\n",
    "\n",
    "data_test  = zip(input_test, labels_test)\n",
    "data_val   = zip(input_val, labels_val)\n",
    "data_train = zip(input_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tensorflow code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import rnn_cell, seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell resets the graphs and session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ops.reset_default_graph()\n",
    "try:\n",
    "    sess.close()\n",
    "except:\n",
    "    \n",
    "    pass\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_seq_length = 16\n",
    "output_seq_length = 15\n",
    "batch_size = 128\n",
    "\n",
    "input_vocab_size = 70\n",
    "output_vocab_size = 28\n",
    "embedding_dim = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As on this page we take our Seq2Seq learner to have the follwing shape:\n",
    "\n",
    "![alt text](https://www.tensorflow.org/versions/r0.7/images/basic_seq2seq.png \"Seq2Seq\")\n",
    "\n",
    "This means the decode_input has to be shifted along by one from the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encode_input = [tf.placeholder(tf.int32, \n",
    "                                shape=(None,),\n",
    "                                name = \"ei_%i\" %i)\n",
    "                                for i in range(input_seq_length)]\n",
    "\n",
    "labels = [tf.placeholder(tf.int32,\n",
    "                                shape=(None,),\n",
    "                                name = \"l_%i\" %i)\n",
    "                                for i in range(output_seq_length)]\n",
    "\n",
    "decode_input = [tf.zeros_like(encode_input[0], dtype=np.int32, name=\"GO\")] + labels[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is the meat of the model, and a lot is happening here under the hood.  We take our cells to be LSTM recurrent units, with dropout between the feed-forward layers.  We take 3 of these stacked as our neural network.  We then run this using the seq2seq.embedding_rnn_seq2seq pattern - this let's us hand the neural network sequences like 1,2,3,2,1 - and the neural network automatically embeds this as a one-hot tensor for us.  \n",
    "\n",
    "Note that we build two networks within the 'decoders' scope.  One of these is using feed_previous = True, the other not.  We set this to False during training, so that even if the learner makes a mistake on a letter - we still give it the correct label in the decoder_inputs.  Since we don't have the real label for the test set, this is set to True, and the decoder takes the letter with maximum probability from the last step of the decoder output.  \n",
    "\n",
    "The decode_output is a tensor of shape (batch_size, output_vocab_size).  We can run softmax on this to get logit scores for each letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "cells = [rnn_cell.DropoutWrapper(\n",
    "        rnn_cell.BasicLSTMCell(embedding_dim), output_keep_prob=keep_prob\n",
    "    ) for i in range(3)]\n",
    "\n",
    "stacked_lstm = rnn_cell.MultiRNNCell(cells)\n",
    "\n",
    "with tf.variable_scope(\"decoders\") as scope:\n",
    "    decode_outputs, decode_state = seq2seq.embedding_rnn_seq2seq(\n",
    "        encode_input, \n",
    "        decode_input, \n",
    "        stacked_lstm, \n",
    "        input_vocab_size, \n",
    "        output_vocab_size,\n",
    "        embedding_dim)\n",
    "    \n",
    "    scope.reuse_variables()\n",
    "    \n",
    "    decode_outputs_test, decode_state_test = seq2seq.embedding_rnn_seq2seq(\n",
    "        encode_input,\n",
    "        decode_input, \n",
    "        stacked_lstm, \n",
    "        input_vocab_size, \n",
    "        output_vocab_size, \n",
    "        embedding_dim,\n",
    "        feed_previous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequence_loss is cross-entropy on the soft max of the decode outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_weights = [tf.ones_like(l, dtype=tf.float32) for l in labels]\n",
    "loss = seq2seq.sequence_loss(decode_outputs, labels, loss_weights, output_vocab_size)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple class for getting random batches and reshaping them properly for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataIterator:\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.iter = self.make_random_iter()\n",
    "        \n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            idxs = self.iter.next()\n",
    "        except StopIteration:\n",
    "            self.iter = self.make_random_iter()\n",
    "            idxs = self.iter.next()\n",
    "        X, Y = zip(*[self.data[i] for i in idxs])\n",
    "        X = np.array(X).T\n",
    "        Y = np.array(Y).T\n",
    "        return X, Y\n",
    "\n",
    "    def make_random_iter(self):\n",
    "        splits = np.arange(self.batch_size, len(self.data), self.batch_size)\n",
    "        it = np.split(np.random.permutation(range(len(self.data))), splits)[:-1]\n",
    "        return iter(it)\n",
    "    \n",
    "train_iter = DataIterator(data_train, 128)\n",
    "val_iter = DataIterator(data_val, 128)\n",
    "test_iter = DataIterator(data_test, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation scores are based on the seq2seq loss, and on the precision - the number of words that the model spells perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def get_feed(X, Y):\n",
    "    feed_dict = {encode_input[t]: X[t] for t in range(input_seq_length)}\n",
    "    feed_dict.update({labels[t]: Y[t] for t in range(output_seq_length)})\n",
    "    return feed_dict\n",
    "\n",
    "def train_batch(data_iter):\n",
    "    X, Y = data_iter.next_batch()\n",
    "    feed_dict = get_feed(X, Y)\n",
    "    feed_dict[keep_prob] = 0.5\n",
    "    _, out = sess.run([train_op, loss], feed_dict)\n",
    "    return out\n",
    "\n",
    "def get_eval_batch_data(data_iter):\n",
    "    X, Y = data_iter.next_batch()\n",
    "    feed_dict = get_feed(X, Y)\n",
    "    feed_dict[keep_prob] = 1.\n",
    "    all_output = sess.run([loss] + decode_outputs_test, feed_dict)\n",
    "    eval_loss = all_output[0]\n",
    "    decode_output = np.array(all_output[1:]).transpose([1,0,2])\n",
    "    return eval_loss, decode_output, X, Y\n",
    "\n",
    "def eval_batch(data_iter, num_batches):\n",
    "    losses = []\n",
    "    predict_loss = []\n",
    "    for i in range(num_batches):\n",
    "        eval_loss, output, X, Y = get_eval_batch_data(data_iter)\n",
    "        losses.append(eval_loss)\n",
    "        \n",
    "        for index in range(len(output)):\n",
    "            real = Y.T[index]\n",
    "            predict = np.argmax(output, axis = 2)[index]\n",
    "            predict_loss.append(all(real==predict))\n",
    "    return np.mean(losses), np.mean(predict_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I simply ran this until it looked like the validation set score was not improving then aborted with a keyboard interrupt.  This took about 20 minutes on my Titan X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interrupted by user\n"
     ]
    }
   ],
   "source": [
    "for i in range(100000):\n",
    "    try:\n",
    "        train_batch(train_iter)\n",
    "        if i % 1000 == 0:\n",
    "            val_loss, val_predict = eval_batch(val_iter, 16)\n",
    "            train_loss, train_predict = eval_batch(train_iter, 16)\n",
    "            print \"val loss   : %f, val predict   = %.1f%%\" %(val_loss, val_predict * 100)\n",
    "            print \"train loss : %f, train predict = %.1f%%\" %(train_loss, train_predict * 100)\n",
    "            print\n",
    "            sys.stdout.flush()\n",
    "    except KeyboardInterrupt:\n",
    "        print \"interrupted by user\"\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Examining model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reach ~ 50% on our hold-out validation set, which may seem low.  Let's see some of the outputs on our test data set, to see the kind of mistakes that the model is making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_loss, output, X, Y = get_eval_batch_data(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pronunciation                            real spelling     model spelling    is correct\n",
      "\n",
      "B-IH0-HH-EY1-V-Y-ER0-AH0-L               behavioral        bbeeaalar         False\n",
      "S-IY1-T-AH0-S                            cetus             setts             False\n",
      "S-AO0-R-OW1-K-AH0                        soroka            sooraa            False\n",
      "P-AA2-B-OY1-L-ER0-Z                      potboilers        ppllrrrss         False\n",
      "B-EY1-T-IY0                              batie             batty             False\n",
      "S-EH1-N-AH0-T-ER0-Z                      senators          senntrs           False\n",
      "AA1-L-AH0-N-D                            olund             lland             False\n",
      "HH-IY1-T-L-IY0                           heatley           heelly            False\n",
      "D-IH2-S-K-AH0-N-S-ER1-T                  disconcert        diisstreen        False\n",
      "V-IH1-K-T-AH0-M-AY0-Z                    victimize         iiiittas          False\n",
      "B-IH1-G-Z-B-IY0                          bigsby            bbbbi             False\n",
      "Y-AE1-N-S-K-IY0                          jansky            annsk             False\n",
      "L-EH1-V-IH0-T-EY2-T-IH0-D                levitated         eeeettted         False\n",
      "AE0-D-V-AY1-Z-IH0-Z                      advises           daaiiss           False\n",
      "F-AO1-ER0-R                              forrer            forrr             False\n",
      "M-AA1-R-T-AH0-N                          marten            martnn            False\n",
      "S-T-R-IH1-K-L-AH0-N-D                    strickland        sttiiilad         False\n",
      "S-T-R-EH1-CH                             stretch           sttrhhh           False\n",
      "K-OW0-V-ER1-T-L-IY0                      covertly          coolltee          False\n",
      "D-AO1-B                                  daube             dobb              False\n",
      "F-IH2-G-Y-ER0-IY1-N                      figurine          fiiiirn           False\n",
      "ER0-Z-AA1-S-AH0                          rzasa             saaass            False\n",
      "P-L-AW1-SH-EY2-R                         plowshare         pollorrh          False\n",
      "V-IY1-N-AH0-S                            venus             vvvns             False\n",
      "AA0-R-M-AA1-T-OW0                        armato            aaaato            False\n",
      "AH0-CH-IY1-V-M-AH0-N-T-S                 achievements      cceeeatnns        False\n",
      "M-EH1-D-AO0                              meddaugh          mmdde             False\n",
      "F-EY1-S-B-UH2-K                          facebook          faaffkk           False\n",
      "T-AE1-F                                  taffe             ttfff             False\n",
      "L-AA1-JH-IH0-NG                          lodging           lllggggg          False\n",
      "V-AY1-B-R-EY0-T                          vibrate           vvvrtt            False\n",
      "B-AO1-F-OW0                              boffo             boooo             False\n",
      "HH-AE1-S-AH0-N                           hassen            haans             False\n",
      "R-EH1-D-L-AY2-N-IH0-NG                   redlining         reeiiinnn         False\n",
      "AE1-L-AH0-N-Z                            allens            aalns             False\n",
      "OW1-V-ER0-S-T-AE2-F                      overstaff         vovvooo           False\n",
      "G-OW1-L-IY0                              goley             golly             False\n",
      "T-UW1-TH                                 tooth             ttthh             False\n",
      "HH-AY1-D-R-AH0-F-OY2-L                   hydrofoil         hhrrdilli         False\n",
      "P-EH1-N-UH0-L                            penuel            ppelln            False\n",
      "S-N-OW1-B-AO2-R-D-Z                      snowboards        soonorres         False\n",
      "K-AH0-P-OW1-L-AH0                        cupola            coolla            False\n",
      "K-AA1-N-F-AH0-D-EH2-N-S-IH0-Z            confidences       connndssse        False\n",
      "D-IH0-R-K-OW1-L-IY0                      dercole           deriliy           False\n",
      "OW2-V-ER0-P-AA1-P-Y-AH0-L-EY0-T-IH0-NG   overpopulating    pooprriiiiiin     False\n",
      "G-IH1-L-B-IY0                            gilbey            gillyy            False\n",
      "P-AA1-K-M-AA2-R-K                        pockmark          pooaack           False\n",
      "P-EY1-V-IH0-NG                           paving            ppiiin            False\n",
      "AH0-N-CH-EH1-K-T                         unchecked         cnccttt           False\n",
      "R-AH0-G-R-EH1-T                          regret            rrrettt           False\n",
      "K-EH1-M-IH0-R                            chemyr            ceemr             False\n",
      "SH-ER0-AA1-R                             sharar            shhrrr            False\n",
      "AH0-N-G-AH1-V-ER0-N-AH0-B-AH0-L          ungovernable      nnnneaarll        False\n",
      "D-IH0-HH-Y-UW1-M-AH0-N-AY0-Z-D           dehumanized       ddddiiiis         False\n",
      "F-R-IY1-L-AE2-N-S-ER0                    freelancer        frrellnes         False\n",
      "S-UW2-M-AA1-T-R-AH0                      sumatra           ssaaaatr          False\n",
      "P-ER0-AA1-Z-AH0                          peraza            ppaaas            False\n",
      "D-EY1-L                                  dayle             ddll              False\n",
      "B-AE1-L-AH0-T-IH0-NG                     balloting         balliing          False\n",
      "D-AA1-P-IH0-S                            dapice            ddpps             False\n",
      "IH0-K-W-IH1-P-T                          equipped          iiiptt            False\n",
      "OW0-V-IH1-T-S                            ovitz             vvits             False\n",
      "S-AH1-M-ER0-AY2-Z-IH0-Z                  summarizes        ssumiiies         False\n",
      "M-L-AA1-D-IH0-CH                         mladic            mlliihh           False\n",
      "K-AA1-Z-B-ER0-G                          kozberg           coobbre           False\n",
      "AH0-N-T-EH1-S-T-IH0-D                    untested          nntttee           False\n",
      "D-EH2-L-AH0-G-EY1-SH-AH0-N-Z             delegations       ddllaiiins        False\n",
      "K-AE1-TH-R-IH0-N                         catharine         caartin           False\n",
      "D-OW0-M-IY0-N-IY1-CH-IY0                 dominici          ddmiiiii          False\n",
      "OW1-V-ER0-P-AW1-R-IH0-NG                 overpowering      ooorrriiin        False\n",
      "S-W-IY1-T-M-AH0-N                        sweatman          seetmmn           False\n",
      "K-IY0-AA0-V-EH1-T-AH0                    chiavetta         ccattaa           False\n",
      "B-EH1-D-R-UW2-M-Z                        bedrooms          beeeoosss         False\n",
      "N-AY1-S-N-AH0-S                          niceness          nnnsss            False\n",
      "K-AH0-T-EH1-L-IY0                        catelli           ccttlly           False\n",
      "K-AA1-L-S-AH0                            khalsa            cllas             False\n",
      "HH-ER1-N-CH-ER0                          hrncir            hhhrreeee         False\n",
      "R-AA1-S-K-AH0                            raska             rross             False\n",
      "AE2-F-R-AH0-D-AY1-T-IY0                  aphrodite         faariiit          False\n",
      "AH0-CH-IY1-V                             achieve           cavie             False\n",
      "T-AH1-S-AH0-L                            tussle            tuull             False\n",
      "Z-AA1-K-M-AH0-N                          zachmann          ccmmmn            False\n",
      "B-UH1-L-F-AY2-T-ER0                      bullfighter       bblltterr         False\n",
      "B-L-OW1-HH-AA2-R-D-Z                     blowhards         blloords          False\n",
      "T-R-AE2-N-Z-K-AE1-P-IH0-T-AH0-L          transcapital      tttaaaaaiiie      False\n",
      "Y-UW2-JH-IY1-N-IY0-AH0                   eugenia           jeniig            False\n",
      "Y-AH0-N-AA1-F-S-K-IY0                    yanofsky          nnnskk            False\n",
      "T-ER1-N-P-AY2-K-S                        turnpikes         teeniiss          False\n",
      "W-UH1-D-SH-EH2-D-Z                       woodsheds         woudddss          False\n",
      "L-ER0-EY1-N                              laraine           llanrr            False\n",
      "AE1-G-R-AH0-G-IH0-T-S                    aggregates        ggaaatss          False\n",
      "M-AH1-S-AH0-L-Z                          muscles           mussll            False\n",
      "Y-EH1-R-AH0-N                            yaron             reann             False\n",
      "S-T-ER1-Z-AH0                            sturza            sttrs             False\n",
      "L-AY1-S-IH0-N-JH-ER0                     leisinger         lliinger          False\n",
      "IH0-K-S-K-L-UW1-D-AH0-B-AH0-L            excludable        ccellllaees       False\n",
      "S-AH0-P-OW1-Z-IH0-NG                     supposing         ssppiing          False\n",
      "R-IH0-V-IH1-R                            revere            rriiee            False\n",
      "R-EH1-Z-AH0-V-W-AA2-R                    reservoir         rrreoose          False\n",
      "AA0-R-N-EH1-T                            arnett            arntt             False\n",
      "P-AO1-R-TH                               porth             poott             False\n",
      "K-AA0-P-OW1-N-IY0                        caponi            cooonn            False\n",
      "W-AO1-L-S-T-AH0-N                        walston           wwlttnn           False\n",
      "S-T-AA1-K-HH-OW2-L-D-ER0-Z               stockholders      ssttaaaores       False\n",
      "CH-IH1-CH-EH0-S-T-ER0                    chichester        cchhtttere        False\n",
      "W-IH1-N-IH0-P-AH0-G                      winnipeg          wiiiiggg          False\n",
      "R-OW1-B-AH0-L-IH0-NG                     robling           rrrliin           False\n",
      "IH1-SH-UW0-ER0                           issuer            shhiier           False\n",
      "AE1-M-B-ER0-G-ER0                        amberger          maarrre           False\n",
      "EH0-M-EH0-L-IY1-T-AH0                    emelita           eeeetaaa          False\n",
      "K-AH0-P-ER1-N-AH0-K-AH0-S                copernicus        coorrnssis        False\n",
      "T-AW1-AH0-L-IH0-NG                       toweling          tttllng           False\n",
      "AE1-K-B-AE0-SH                           akbash            aaackk            False\n",
      "IH2-K-S-T-R-EH1-M-IH0-Z-AH0-M            extremism         iiittmmmss        False\n",
      "S-L-AA1-G-D                              slogged           slldd             False\n",
      "G-IH1-L-D-ER0                            guilder           gllierd           False\n",
      "JH-ER0-EY1                               gerais            gaaeer            False\n",
      "D-IH0-K-T-EY1-T-ER0-SH-IH2-P-S           dictatorships     ddittccciiirs     False\n",
      "R-UW1-D-IY0                              ruedy             rrddy             False\n",
      "EH1-D-AH0-T-ER0                          editor            deettr            False\n",
      "D-IY1-CH                                 dietsch           decch             False\n",
      "D-AW1-R-Z                                dowers            doors             False\n",
      "B-UW0-K-EY1                              bouquet           bakkk             False\n",
      "CH-EY1-NG                                chseing           caain             False\n",
      "M-IH1-N-IY0                              minnie            miiin             False\n",
      "D-AE1-SH-B-AO2-R-D                       dashboard         daaadrrh          False\n",
      "K-AE1-P-F-ER0                            kapfer            papprr            False\n",
      "S-EH1-L-N-IH0-K                          celnik            seliin            False\n"
     ]
    }
   ],
   "source": [
    "print \"pronunciation\".ljust(40),\n",
    "print \"real spelling\".ljust(17),\n",
    "print \"model spelling\".ljust(17),\n",
    "print \"is correct\"\n",
    "print\n",
    "\n",
    "for index in range(len(output)):\n",
    "    phonemes = \"-\".join([index_to_phoneme[p] for p in X.T[index]]) \n",
    "    real = [index_to_letter[l] for l in Y.T[index]] \n",
    "    predict = [index_to_letter[l] for l in np.argmax(output, axis = 2)[index]]\n",
    "   \n",
    "    print phonemes.split(\"-_\")[0].ljust(40),\n",
    "    print \"\".join(real).split(\"_\")[0].ljust(17),\n",
    "    print \"\".join(predict).split(\"_\")[0].ljust(17),\n",
    "    print str(real == predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
